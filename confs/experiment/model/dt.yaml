name: dt
framework: sklearn
model_type: tabular

model_tasks:
  - classification

label_types:
  - binary
  - discrete

set_params_function:
  _target_: CMC_utils.miscellaneous.do_nothing

init_params:
  _target_: sklearn.tree.DecisionTreeClassifier
  _convert_: all

  criterion: gini
  # The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see Mathematical formulation.
  # {“gini”, “entropy”, “log_loss”}, default=”gini”

  splitter: best
  # The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
  # {“best”, “random”}, default=”best”

  max_depth:
  # The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
  # int, default=None

  min_samples_split: 2
  # The minimum number of samples required to split an internal node:
  # int or float, default=2
  # - If int, then consider min_samples_split as the minimum number.
  # - If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.

  min_samples_leaf: 1
  # The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.
  # int or float, default=1
  # - If int, then consider min_samples_leaf as the minimum number.
  # - If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.

  min_weight_fraction_leaf: 0.0
  # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
  # float, default=0.0

  random_state: ${seed}
  # Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to "best". When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details.
  # int, RandomState instance or None, default=None

  max_leaf_nodes:
  # Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
  # int, default=None

  min_impurity_decrease: 0.0
  # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
  # float, default=0.0
  # The weighted impurity decrease equation is the following:
  #
  # N_t / N * (impurity - N_t_R / N_t * right_impurity
  #                    - N_t_L / N_t * left_impurity)
  # where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.
  #
  # N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.
  
  class_weight: balanced
  # Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.
  # dict, list of dict or “balanced”, default=None
  # Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].
  # The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))
  # For multi-output, the weights of each column of y will be multiplied.
  # Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.
  
  ccp_alpha: 0.0
  # Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details.
  # non-negative float, default=0.0

fit_params: {}

train_function:
  _target_: CMC_utils.models.train_sklearn_model

test_function:
  _target_: CMC_utils.models.test_sklearn_model

save_function:
  _target_: CMC_utils.save_load.save_model

file_extension: pkl

load_function:
  _target_: CMC_utils.save_load.load_model